{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univar_gauss(m,s):\n",
    "    U=0                         #initialise\n",
    "    for i in range(12):\n",
    "        U+=np.random.uniform()\n",
    "    U-=6\n",
    "    U*=math.sqrt(s)\n",
    "    U+=m\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(n,mx1, vx1, my1, vy1, mx2, vx2, my2, vy2):\n",
    "    D1=[]\n",
    "    D2=[]\n",
    "    for i in range(n):\n",
    "        D1.append((univar_gauss(mx1,vx1),univar_gauss(my1,vy1)))\n",
    "    for i in range(n):\n",
    "        D2.append((univar_gauss(mx2,vx2),univar_gauss(my2,vy2)))\n",
    "    #print(\"D1: \"+str(D1))\n",
    "    #print(\"D2: \"+str(D2))\n",
    "    D3=D1+D2\n",
    "    Dmat = np.empty([2*n,2])\n",
    "    for i in range(len(D3)):\n",
    "        Dmat[i][0]=D3[i][0]\n",
    "        Dmat[i][1]=D3[i][1]\n",
    "    \n",
    "    \n",
    "log_reg(100,1,1,1,1,5,5,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:97: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:98: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features 2\n",
      "Optimized weights [[0.19582749 0.2006406 ]]\n",
      "Optimized intercept -0.16041272067953397\n",
      "[[ 52   1]\n",
      " [186 251]]\n",
      "Test Accuracy: 0.6183673469387755\n",
      "Precision: 0.996031746031746\n",
      "Sensitivity: 0.9811320754716981\n",
      "Specificity: 0.5743707093821511\n",
      "==Ski-learn==\n",
      "[[231   7]\n",
      " [ 15 237]]\n",
      "Accuracy: 0.9551020408163265\n",
      "Precision: 0.9713114754098361\n",
      "Sensitivity: 0.9705882352941176\n",
      "Specificity: 0.9404761904761905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/3nqing/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import scipy\n",
    "import numpy as np\n",
    "import struct as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression     \n",
    "\n",
    "def univariateGenerator(expectation, variance):\n",
    "    u = scipy.random.uniform()\n",
    "    v = scipy.random.uniform()\n",
    "    \n",
    "    z = scipy.sqrt(-2.0*scipy.log(u))*scipy.cos(2.0*scipy.pi*v)\n",
    "    x = scipy.sqrt(variance)*z + expectation\n",
    "#    w = scipy.sqrt(-2.0*scipy.log(u))*scipy.sin(2.0*scipy.pi*v)\n",
    "#    y = scipy.sqrt(variance)*w + expectation\n",
    "    return x\n",
    "\n",
    "def genData(n, mx1, vx1, my1, vy1, mx2, vx2, my2, vy2):\n",
    "    with open('hw4regression.csv', mode = 'w') as data:\n",
    "        writer = csv.writer(data,delimiter = ',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        writer.writerow(['label','x','y'])\n",
    "        a,b = n,n\n",
    "        while a!=0 and b !=0:\n",
    "            k = random.randint(0,1)\n",
    "            if k == 0 and a >0:\n",
    "                writer.writerow([0,univariateGenerator(mx1,vx1), univariateGenerator(my1,vy1)])\n",
    "                a -=1\n",
    "            elif k ==1 and b > 0:\n",
    "                writer.writerow([1,univariateGenerator(mx2,vx2), univariateGenerator(my2,vy2)])            \n",
    "                b-=1\n",
    "def weightInitialization(n_features):\n",
    "    w = np.zeros((1,n_features))\n",
    "    b = 0\n",
    "    return w,b \n",
    "def sigmoid_activation(result):\n",
    "    final_result = 1/(1+np.exp(-result))\n",
    "    return final_result\n",
    "def model_optimize(w, b, X, Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Prediction\n",
    "    final_result = sigmoid_activation(np.dot(w,X.T)+b)\n",
    "    Y_T = Y.T\n",
    "    cost = (-1/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))\n",
    "    #\n",
    "    \n",
    "    #Gradient calculation\n",
    "    dw = (1/m)*(np.dot(X.T, (final_result-Y.T).T))\n",
    "    db = (1/m)*(np.sum(final_result-Y.T))\n",
    "    \n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "def predict(final_pred, m):\n",
    "    y_pred = np.zeros((1,m))\n",
    "    for i in range(final_pred.shape[1]):\n",
    "        if final_pred[0][i] > 0.5:\n",
    "            y_pred[0][i] = 1\n",
    "    return y_pred\n",
    "def model_predict(w, b, X, Y, learning_rate, no_iterations):\n",
    "    costs = []\n",
    "    for i in range(no_iterations):\n",
    "        #\n",
    "        grads, cost = model_optimize(w,b,X,Y)\n",
    "        #\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        #weight update\n",
    "        w = w - (learning_rate * (dw.T))\n",
    "        b = b - (learning_rate * db)\n",
    "        #\n",
    "        \n",
    "        if (i % 100 == 0):\n",
    "            costs.append(cost)\n",
    "            #print(\"Cost after %i iteration is %f\" %(i, cost))\n",
    "    \n",
    "    #final parameters\n",
    "    coeff = {\"w\": w, \"b\": b}\n",
    "    gradient = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return coeff, gradient, costs\n",
    "def LR():\n",
    "    col_names = ['label','x','y']\n",
    "    data = pd.read_csv(\"hw4regression.csv\",header=0, names=col_names) \n",
    "    feature_cols =['x','y']\n",
    "    X = data[feature_cols]\n",
    "    y = data.label\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "    logreg = LogisticRegression()\n",
    "    X_tr_arr = X_train\n",
    "    X_ts_arr = X_test\n",
    "    y_tr_arr = y_train.as_matrix()\n",
    "    y_ts_arr = y_test.as_matrix()\n",
    "    #Get number of features\n",
    "    n_features = X_tr_arr.shape[1]\n",
    "    print('Number of Features', n_features)\n",
    "    w, b = weightInitialization(n_features)\n",
    "    #Gradient Descent\n",
    "    coeff, gradient, costs = model_predict(w, b, X_tr_arr, y_tr_arr, learning_rate=0.0001,no_iterations=10000)\n",
    "    #Final prediction\n",
    "    w = coeff[\"w\"]\n",
    "    b = coeff[\"b\"]\n",
    "    print('Optimized weights', w)\n",
    "    print('Optimized intercept',b)\n",
    "    #\n",
    "#    final_train_pred = sigmoid_activation(np.dot(w,X_tr_arr.T)+b)\n",
    "    final_test_pred = sigmoid_activation(np.dot(w,X_ts_arr.T)+b)\n",
    "    #\n",
    "#    m_tr =  X_tr_arr.shape[0]\n",
    "    m_ts =  X_ts_arr.shape[0]\n",
    "#    y_tr_pred = predict(final_train_pred,m_tr)\n",
    "    y_ts_pred = predict(final_test_pred, m_ts)\n",
    "\n",
    "    cnf_matrix = metrics.confusion_matrix(y_ts_pred.T, y_ts_arr)\n",
    "    print(cnf_matrix)\n",
    "    print(\"Test Accuracy:\",metrics.accuracy_score(y_ts_pred.T, y_ts_arr))\n",
    "    print(\"Precision:\",metrics.precision_score(y_ts_pred.T, y_ts_arr))\n",
    "    print(\"Sensitivity:\",cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1]))\n",
    "    print(\"Specificity:\",cnf_matrix[1][1]/(cnf_matrix[1][0]+cnf_matrix[1][1]))\n",
    "    \n",
    "    # fit the model with data\n",
    "    logreg.fit(X_train,y_train)\n",
    "    #\n",
    "    y_pred=logreg.predict(X_test)\n",
    "    print(\"==Ski-learn==\")\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print(cnf_matrix)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "    #sensitivity = The proportion of observed positives that were predicted to be positive \n",
    "    print(\"Sensitivity:\",cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1]))\n",
    "    print(\"Specificity:\",cnf_matrix[1][1]/(cnf_matrix[1][0]+cnf_matrix[1][1]))\n",
    "#    specificity = The proportion of observed negatives that were predicted to be negatives.\n",
    "LR()\n",
    "#genData(1000,1,1,1,1,5,5,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
